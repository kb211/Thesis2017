{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is meant to explain how model V.3 works. In order to test if the implementation works, I generate to create some fake data according to some probabilities that I also made up. The goal is to omit the parameters and to re-learn them from the generated data via MLE and EM.\n",
    "\n",
    "The whole network is made up of four variables F (fraud/non-fraud), X (input variables), C (hidden variable that should explain X) and ID (the ID of the person that made the transaction)\n",
    "\n",
    "The network looks like this: F --> X <-- C <-- ID. X is conditioned on F and C and C on id. \n",
    "The joint probability equates to: p(ID)p(F)p(ID|C)p(X|F,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import EM_v3 as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assert that the conditional probability formulation is correct, by checking that for every id, the probabilities of c sum to 1.\n",
      "[ 1.  1.  1.  1.  1.]\n",
      "marginal probability for c: \n",
      "[ 0.39  0.28  0.33]\n",
      "joint probability for c and f: \n",
      "[[ 0.273  0.117]\n",
      " [ 0.196  0.084]\n",
      " [ 0.231  0.099]]\n",
      "Assert that the conditional probability formulation is correct, by checking that for every f, c, the probabilities of x sum to 1.\n",
      "[[ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#marginal probabilities\n",
    "p_ids = np.array([0.1, 0.2, 0.3, 0.2, 0.2])\n",
    "p_f = np.array([.7, .3])\n",
    "\n",
    "#p(C|ID)\n",
    "c_given_id = np.array([[0.1, 0.4, 0.4, 0.6, 0.3],\n",
    "              [0.1, 0.4, 0.3, 0.2, 0.3],\n",
    "              [0.8, 0.2, 0.3, 0.2, 0.4]])\n",
    "print \"Assert that the conditional probability formulation is correct, by checking that for every id, the probabilities of c sum to 1.\"\n",
    "print np.sum(c_given_id, axis=0)\n",
    "\n",
    "p_c = np.sum(p_ids*c_given_id, axis=1)\n",
    "\n",
    "p_c_f = p_c[:, np.newaxis]*p_f[:, np.newaxis].T\n",
    "\n",
    "#p(X|F,ID)\n",
    "x_given_c_f = np.array([[[ 0.2,  0.2],\n",
    "               [ 0.2,  0.2],\n",
    "               [ 0.5,  0.1]],\n",
    "\n",
    "               [[ 0.15,  0.6],\n",
    "               [ 0.2,  0.1],\n",
    "               [ 0.2,  0.2]],\n",
    "\n",
    "               [[0.1, 0.1],\n",
    "                [0.2, 0.6],\n",
    "                [0.2, 0.2]],\n",
    "\n",
    "               [[0.55, 0.1],\n",
    "                [0.4, 0.1],\n",
    "                [0.1, 0.5]]\n",
    "               ])\n",
    "print \"marginal probability for c: \"\n",
    "print p_c\n",
    "print \"joint probability for c and f: \"\n",
    "print p_c_f\n",
    "print \"Assert that the conditional probability formulation is correct, by checking that for every f, c, the probabilities of x sum to 1.\"\n",
    "print np.sum(x_given_c_f, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "These two methods generate the data with our previously created parameters.\n",
    "'''\n",
    "def one_hot(array):\n",
    "    out = np.zeros((array.shape[0], np.max(array)+1))\n",
    "    out[np.arange(array.shape[0]), array] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def simulate(p_ids, p_f, c_given_id, p_c_f, x_given_c_f, n):\n",
    "\n",
    "    F = np.random.choice(p_f.shape[0], n, p=p_f)\n",
    "\n",
    "    IDs = np.random.choice(p_ids.shape[0], n, p=p_ids)\n",
    "\n",
    "    C = np.array([np.random.choice(c_given_id.shape[0], p=c_given_id[:, ID]) for ID in IDs])\n",
    "\n",
    "    X = np.array((np.array([x_given_c_f[:, c, f] for f, c in zip(F, C)]) > np.random.random((n, x_given_c_f.shape[0]))) * 1.0)\n",
    "\n",
    "    IDs = one_hot(IDs)\n",
    "    F = one_hot(F)\n",
    "    C = one_hot(C)\n",
    "\n",
    "    return X, F, IDs, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the maximum likelihood estimation function that learns p(X|F)\n",
    "'''\n",
    "def mle(X, F):\n",
    "    p_f = np.mean(F, axis=0)\n",
    "    x_given_f = np.dot(F.T, X)\n",
    "    x_given_f /= (np.sum(x_given_f, axis=1)[:, None] )\n",
    "\n",
    "    return p_f,  x_given_f.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assert that x_given_f_learned is true x_given_f\n",
      "true x_given_f:\n",
      "[[ 0.299   0.167 ]\n",
      " [ 0.1805  0.328 ]\n",
      " [ 0.161   0.273 ]\n",
      " [ 0.3595  0.232 ]]\n",
      "learned x_given_f:\n",
      "[[ 0.30551953  0.15425356]\n",
      " [ 0.18403228  0.3366435 ]\n",
      " [ 0.15405678  0.26713009]\n",
      " [ 0.35639141  0.24197286]]\n",
      "assert that x_given_c_learned is true x_given_c\n",
      "true x_given_c:\n",
      "[[ 0.2    0.2    0.38 ]\n",
      " [ 0.285  0.17   0.2  ]\n",
      " [ 0.1    0.32   0.2  ]\n",
      " [ 0.415  0.31   0.22 ]]\n",
      "leaned x_given_c:\n",
      "[[ 0.31460139  0.33686354  0.17690271]\n",
      " [ 0.13120109  0.44534819  0.16258303]\n",
      " [ 0.38754802  0.10560747  0.11054835]\n",
      " [ 0.1666495   0.1121808   0.54996591]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This part simulates data with made up parameters. \n",
    "\"C_hidden\" is an array with random values that sum to 1 for each data point. \n",
    "This is done to \"hide\" the values of C in order to guess them via EM.\n",
    "'''\n",
    "X, F, IDs, C = simulate(p_ids, p_f, c_given_id, p_c_f, x_given_c_f, 10000)\n",
    "\n",
    "C_hidden = one_hot(np.random.choice(3, 10000, p=(np.arange(3)+7.)/np.sum(np.arange(3)+7.)))\n",
    "\n",
    "expmax = em.expectation_maximization(0, 0)\n",
    "\n",
    "\n",
    "\n",
    "p_c_learned, x_given_c_learned, c_given_id_learned = expmax.em_algorithm(C_hidden, X, IDs, 400)\n",
    "\n",
    "p_f_learned, x_given_f_learned = mle(X, F)\n",
    "\n",
    "print \"assert that x_given_f_learned is true x_given_f\"\n",
    "print \"true x_given_f:\"\n",
    "print np.sum(x_given_c_f*p_c_f, axis=1)/p_f\n",
    "print \"learned x_given_f:\"\n",
    "print x_given_f_learned\n",
    "\n",
    "print \"assert that x_given_c_learned is true x_given_c\"\n",
    "print \"true x_given_c:\"\n",
    "print np.sum(x_given_c_f*p_c_f, axis=2)/p_c\n",
    "print \"leaned x_given_c:\"\n",
    "print x_given_c_learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current state of affairs is that the EM algorithm needs to be modified a little to learn X <-- C <-- ID. Currently parameters p(X|C) are a bit off as you can see. The next step which I am working on is to infer p(F|X, ID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "update: \n",
    "# E-step\n",
    "\n",
    "With N data points the dimensions of the input matrix X is (N, D), for C the dimensions are (N, K) and for id the dimensions are (N, (number of id's)).\n",
    "\n",
    "In the original EM algorithm the E-step was defined as:\n",
    "$p(C \\mid X) = \\frac{p(X \\mid C) p(C)}{\\sum_{k=1}^{K} p(X \\mid c_k) p(c_k))} = \\frac{p(X \\mid C) p(C)}{p(X)}$\n",
    "\n",
    "where $p(X \\mid C) = \\prod_{d}^{D} p(x_d \\mid C)$\n",
    "\n",
    "Since we now have a joint probability defined as $p(X \\mid C) p(C \\mid ID) p(ID)$, we had to modify the E-step such that we find $p(C \\mid X, ID)$: \n",
    "\n",
    "Since the id is known for each transaction we can multiply $ ID \\cdot p(C \\mid ID) $, where ID is was the list of id's for each transaction. This results in a (N, K) matrix where for each N we have the associated probability $p(C \\mid ID = id)$. I am not entirely sure about this. However, I think that this allows me to replace $p(X \\mid C) p(C \\mid ID) p(ID)$ with $p(X \\mid C) p(C \\mid ID =id)$  \n",
    "\n",
    "The toal E-step update rule is then \n",
    "$\\frac{p(X \\mid C) p(C \\mid ID =id)}{\\sum_{k=1}^{K} p(X \\mid c_k) p(c_k \\mid ID =id)}$\n",
    "\n",
    "# M-step\n",
    "\n",
    "The M step is very straightforward. \n",
    "$p(id)$ is known a priori (just count how many times each id occurs and divide by total amount of transactions).\n",
    "\n",
    "$p(X|C)$ is calculated by dividing $\\frac{\\text{# co-occurences of x_d and c_k}}{\\text{# occurences of c_k}}$.\n",
    "\n",
    "$p(C|ID)$ is calculated by dividing $\\frac{\\text{# co-occurences of c_k and ID=id}}{p(id)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
